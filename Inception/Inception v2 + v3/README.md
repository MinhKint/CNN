# I. Why an improved version is needed

While Inception V1 brought many performance improvements, it also had some major limitations. The most notable one was that the model size and computational cost increased rapidly as the network expanded. If we simply wanted to increase the accuracy by increasing the filter size, this would result in a disproportionate increase in computational cost and parameter count (~4x), with only modest benefits in many cases. This is not reasonable in a practical context, where computational efficiency and low parameter count are still factors supporting many different use cases such as mobile phones and big data.

So inception v2 and v3 were born to answer the question of ‘how to scale neural networks without increasing computational cost unreasonably’ by using new optimization techniques, such as factorized convolutions 

# II. Inception v2 v3 architectural issues that the paper does not explicitly address

- **Inception modules do not specify the number of kernels:** For example, with Inception modules from figure 5, 6, 4 kernel number values ​​are needed corresponding to 4 branches in each module. This makes the paper reader need to test the kernel value combinations: Are these 4 values ​​equal or different? If different, how much is the difference and is it much different from the 1x1 convolution? - Input size in the transfer between Inception modules: Example with input size for 3 Inception modules figure 5. Because the paper mentions the use of Grid Size Reduction to transfer the size and number of features between Inception modules figure 5 and figure 6; between Inception module figure 6 and figure 7. Therefore, the reader is not sure whether all 3 Inception modules in figure have 2 modules that still keep the input size of 35 x 35 x288 or all 3 more
- **Unclear about pooling:** The paper mentions that the reader can freely use between max pooling or average pooling in each Inception module and in the Grid Size Reduction section
- **Unclear in the classification between Inception v3 and v2:** The paper says that the best version of Inception v2 (using Batch Normalization in the Auxiliary port) is Inception v3. And the reader is not sure whether Batch Normalization is only implemented in the auxiliary port or after all convolution layers because the reader sees a lot of reference code on the internet (including Inception v3 in tensorflow using Batch Normalization after each convolution layer)

- **In summary:** I feel that the author of the paper made a mistake or an omission, which led to misunderstanding and difficulty in reading the paper.